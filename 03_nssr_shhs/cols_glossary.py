
GLOSSARY_TERMS = [
    {"es": "Aumento de Datos", "en": "Data Augmentation", "desc": "Técnica para aumentar la diversidad de datos de entrenamiento mediante transformaciones aleatorias (como máscaras de tiempo/frecuencia)."},
    {"es": "Aprendizaje Profundo", "en": "Deep Learning", "desc": "Subcampo del ML basado en redes neuronales artificiales con múltiples capas para aprender representaciones de datos."},
    {"es": "Arquitectura ConvNeXt", "en": "ConvNeXt Architecture", "desc": "Red neuronal convolucional moderna que adopta principios de diseño de los Vision Transformers (kernels grandes, capas invertidas)."},
    {"es": "Checkpoint", "en": "Checkpoint", "desc": "Archivo que guarda el estado exacto del modelo (pesos y configuración) en un momento específico del entrenamiento."},
    {"es": "Clasificación de Etapas de Sueño", "en": "Sleep Stage Classification", "desc": "Proceso de asignar una etiqueta (N1, N2, N3, REM, Wake) a una época de sueño basada en señales fisiológicas."},
    {"es": "Conjunto de Datos", "en": "Dataset", "desc": "Colección estructurada de datos utilizada para el entrenamiento, validación y prueba del modelo."},
    {"es": "Conjunto de Entrenamiento", "en": "Training Set", "desc": "Subconjunto de datos utilizado para ajustar los pesos del modelo mediante backpropagation."},
    {"es": "Conjunto de Validación", "en": "Validation Set", "desc": "Subconjunto de datos reservado para evaluar el rendimiento del modelo durante el entrenamiento y ajustar hiperparámetros."},
    {"es": "Cargador de Datos", "en": "DataLoader", "desc": "Utilidad de PyTorch que maneja la carga eficiente de datos en lotes (minibatches) y el barajado (shuffling)."},
    {"es": "Desajuste", "en": "Underfitting", "desc": "Situación donde el modelo es demasiado simple para capturar los patrones subyacentes en los datos."},
    {"es": "Dropout", "en": "Dropout", "desc": "Técnica de regularización donde neuronas seleccionadas aleatoriamente son ignoradas durante el entrenamiento para prevenir sobreajuste."},
    {"es": "Exactitud", "en": "Accuracy", "desc": "Fracción de predicciones que el modelo realizó correctamente respecto al total de muestras."},
    {"es": "Época", "en": "Epoch", "desc": "Un pase completo del algoritmo de aprendizaje a través de todo el conjunto de datos de entrenamiento."},
    {"es": "Espectrograma", "en": "Spectrogram", "desc": "Representación visual del espectro de frecuencias de una señal a lo largo del tiempo, usada como entrada para el modelo."},
    {"es": "F1-Score", "en": "F1-Score", "desc": "Media armónica de la precisión y el recall. Métrica robusta para conjuntos de datos desbalanceados."},
    {"es": "Función de Pérdida", "en": "Loss Function", "desc": "Métrica que cuantifica la diferencia entre las predicciones del modelo y las etiquetas reales (ej. CrossEntropy)."},
    {"es": "Google Cloud Storage (GCS)", "en": "Google Cloud Storage", "desc": "Servicio de almacenamiento de objetos en la nube de Google, usado para alojar el dataset masivo SHHS."},
    {"es": "GPU", "en": "GPU", "desc": "Unidad de Procesamiento Gráfico. Hardware especializado usado para acelerar masivamente las operaciones matriciales del entrenamiento."},
    {"es": "Hipnograma", "en": "Hypnogram", "desc": "Gráfico clínico estándar que representa la progresión de las etapas del sueño a lo largo del tiempo."},
    {"es": "Inferencia", "en": "Inference", "desc": "El proceso de usar un modelo ya entrenado para predecir etiquetas en datos nuevos nunca antes vistos."},
    {"es": "Manifiesto", "en": "Manifest", "desc": "Archivo CSV que actúa como índice maestro, listando explícitamente todos los archivos válidos para asegurar reproducibilidad."},
    {"es": "Muestreador Ponderado", "en": "WeightedRandomSampler", "desc": "Técnica de muestreo que selecciona ejemplos de clases minoritarias con mayor frecuencia para corregir desequilibrios."},
    {"es": "Normalización", "en": "Normalization", "desc": "Preprocesamiento que ajusta los valores de los datos a una escala común (media 0, desviación estándar 1) para facilitar el aprendizaje."},
    {"es": "Punto de Control", "en": "Checkpoint", "desc": "Ver 'Checkpoint'."},
    {"es": "Precisión", "en": "Precision", "desc": "La proporción de identificaciones positivas que fueron realmente correctas."},
    {"es": "Sensibilidad / Recall", "en": "Recall", "desc": "La proporción de positivos reales que fueron identificados correctamente por el modelo."},
    {"es": "Sobreajuste", "en": "Overfitting", "desc": "Error de modelado donde la función se ajusta demasiado a los datos de entrenamiento y falla en generalizar."},
    {"es": "Streamlit", "en": "Streamlit", "desc": "Biblioteca de Python de código abierto utilizada para crear esta aplicación web interactiva de Data Science."},
    {"es": "Tamaño del Lote", "en": "Batch Size", "desc": "Número de muestras de entrenamiento procesadas juntas en una sola iteración de actualización de pesos."},
    {"es": "Tasa de Aprendizaje", "en": "Learning Rate", "desc": "Hiperparámetro que controla cuánto se ajustan los pesos del modelo en respuesta al error estimado en cada paso."},
    {"es": "Tensor", "en": "Tensor", "desc": "Estructura de datos multidimensional fundamental en PyTorch, optimizada para cálculos en GPU."},
    {"es": "Validación Cruzada", "en": "Cross-Validation", "desc": "Técnica para evaluar modelos dividiendo los datos en subconjuntos rotativos de prueba y entrenamiento."},
    {"es": "Verdad fundamental", "en": "Ground Truth", "desc": "La etiqueta o clasificación real 'correcta' proporcionada por expertos humanos (polisomnógrafos)."},
    {"es": ".edf (European Data Format)", "en": ".edf (European Data Format)", "desc": "Formato de archivo estándar médico para series temporales de poligrafía y EEG. Contiene grabaciones digitales de señales biológicas."},
    {"es": ".bdf (BioSemi Data Format)", "en": ".bdf (BioSemi Data Format)", "desc": "Versión de 24 bits del formato EDF, utilizada para grabaciones de mayor resolución. Soportado nativamente por la aplicación."},
    {"es": ".parquet", "en": ".parquet", "desc": "Formato de almacenamiento en columnas optimizado para analítica. Usado en esta aplicación para almacenar espectrogramas procesados de manera eficiente."},
]
