{"cells":[{"cell_type":"code","source":["# ==============================================================================\n","# 1. SETUP AND DEPENDENCY INSTALLATION\n","# ==============================================================================\n","print(\"Ensuring PyTorch Lightning and other libraries are installed...\")\n","# Install the necessary libraries, including gcsfs for Google Cloud Storage access\n","!pip install --upgrade -q pytorch-lightning timm \"pandas==2.2.2\" \"pyarrow==19.0.0\" gcsfs\n","print(\"âœ… Installation check complete.\")\n","\n","# ==============================================================================\n","# 2. IMPORTS AND INITIAL CONFIGURATION\n","# ==============================================================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import timm\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import CSVLogger\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import os\n","\n","# Set matrix multiplication precision for A100/H100 GPUs for better performance\n","torch.set_float32_matmul_precision('medium')\n","print(\"âœ… Libraries imported and configuration set.\")\n","\n","# ==============================================================================\n","# 3. AUTHENTICATE FOR GOOGLE CLOUD STORAGE (GCS)\n","# ==============================================================================\n","from google.colab import auth\n","import gcsfs\n","\n","print(\"Authenticating to Google Cloud...\")\n","# This command will trigger a pop-up to authenticate your user account.\n","auth.authenticate_user()\n","print(\"âœ… Authentication complete.\")\n","\n","\n","# ==============================================================================\n","# 4. MODEL ARCHITECTURE DEFINITION (MULTI-MODEL SUPPORT)\n","# ==============================================================================\n","def get_model(model_name='convnext_base', num_classes=5, pretrained=True):\n","    \"\"\"\n","    Creates a model adapted for sleep stage classification.\n","    Supports multiple architectures like ConvNeXT and Vision Transformer (ViT).\n","\n","    Args:\n","        model_name (str): The name of the model architecture to use.\n","                          Options: 'convnext_base', 'vit_base'.\n","        num_classes (int): The number of output classes.\n","        pretrained (bool): Whether to load pre-trained weights.\n","\n","    Returns:\n","        A PyTorch nn.Module representing the adapted model.\n","    \"\"\"\n","    if model_name == 'convnext_base':\n","        # --- Create ConvNeXT Base Model ---\n","        model = timm.create_model('convnextv2_base.fcmae_ft_in22k_in1k', pretrained=pretrained)\n","\n","        # Adapt the first layer for 1-channel input\n","        original_conv = model.stem[0]\n","        new_first_conv = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","        with torch.no_grad():\n","            if original_conv.weight.shape[1] == 3:\n","                new_first_conv.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","        model.stem[0] = new_first_conv\n","\n","        # Adapt the final classification layer\n","        num_ftrs = model.head.fc.in_features\n","        model.head.fc = nn.Linear(num_ftrs, num_classes)\n","        print(f\"âœ… ConvNeXT Base model created.\")\n","\n","    elif model_name == 'vit_base':\n","        # --- Create Vision Transformer Base Model ---\n","        # Note: ViT is sensitive to image size. Timm handles this well, but performance\n","        # is best when input size is close to the pre-training size (e.g., 224x224).\n","        # We are using our native 76x60 size.\n","        model = timm.create_model('vit_base_patch16_224.augreg_in21k', pretrained=pretrained, img_size=(76, 60))\n","\n","        # Adapt the first layer (patch embedding) for 1-channel input\n","        original_conv = model.patch_embed.proj\n","        new_patch_embed = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","        with torch.no_grad():\n","            if original_conv.weight.shape[1] == 3:\n","                new_patch_embed.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","        model.patch_embed.proj = new_patch_embed\n","\n","        # Adapt the final classification layer\n","        num_ftrs = model.head.in_features\n","        model.head = nn.Linear(num_ftrs, num_classes)\n","        print(f\"âœ… Vision Transformer (ViT) Base model created.\")\n","\n","    else:\n","        raise ValueError(f\"Model '{model_name}' not supported. Choose 'convnext_base' or 'vit_base'.\")\n","\n","    return model\n","\n","print(\"âœ… `get_model` function defined with multi-architecture support.\")\n","\n","# ==============================================================================\n","# 5. PYTORCH LIGHTNING MODULE\n","# ==============================================================================\n","class SleepStageClassifierLightning(pl.LightningModule):\n","    \"\"\"\n","    PyTorch Lightning module for sleep stage classification.\n","    \"\"\"\n","    def __init__(self, model_name, learning_rate=1e-5, class_weights=None):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = get_model(model_name=self.hparams.model_name, num_classes=5, pretrained=True)\n","        self.train_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.val_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.train_f1 = MulticlassF1Score(num_classes=5, average='macro')\n","        self.val_f1 = MulticlassF1Score(num_classes=5, average='macro')\n","        self.weights = torch.tensor(class_weights, dtype=torch.float) if class_weights is not None else None\n","        self.loss_fn = nn.CrossEntropyLoss(weight=self.weights)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        y_pred_logits = self(x)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_acc', self.train_accuracy(y_pred_logits, y_true), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_f1', self.train_f1(y_pred_logits, y_true), on_step=False, on_epoch=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        y_pred_logits = self(x)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', self.val_accuracy(y_pred_logits, y_true), on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_f1', self.val_f1(y_pred_logits, y_true), on_epoch=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n","        scheduler = {\n","            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3),\n","            'monitor': 'val_loss',\n","            'interval': 'epoch',\n","            'frequency': 1,\n","        }\n","        return [optimizer], [scheduler]\n","\n","print(\"âœ… `SleepStageClassifierLightning` module defined.\")\n","\n","# ==============================================================================\n","# 6. OPTIMIZED CUSTOM DATASET DEFINITION WITH METADATA CACHING\n","# ==============================================================================\n","class OptimizedCombinedDataset(Dataset):\n","    \"\"\"\n","    An optimized dataset class that scans for epoch counts only once and saves\n","    the results to a metadata file for near-instantaneous loading on future runs.\n","    \"\"\"\n","    def __init__(self, file_paths, metadata_path):\n","        self.file_paths = file_paths\n","        self.metadata_path = metadata_path\n","        self._cache = {}\n","        self.fs = gcsfs.GCSFileSystem()\n","\n","        if self.fs.exists(self.metadata_path):\n","            print(f\"Found metadata file at {self.metadata_path}. Loading epoch counts...\")\n","            with self.fs.open(self.metadata_path, 'r') as f:\n","                metadata_df = pd.read_csv(f)\n","            epoch_counts_map = dict(zip(metadata_df['filepath'], metadata_df['epoch_count']))\n","            self.epochs_per_file = [epoch_counts_map.get(fp, 0) for fp in self.file_paths]\n","            print(\"âœ… Epoch counts loaded from metadata file.\")\n","        else:\n","            print(f\"Metadata file not found. Performing one-time scan of {len(self.file_paths)} files...\")\n","            self.epochs_per_file = []\n","            epoch_data = []\n","            for f_path in self.file_paths:\n","                try:\n","                    df_labels = pd.read_parquet(f_path, columns=['label'])\n","                    num_valid = df_labels['label'].isin([0, 1, 2, 3, 4]).sum()\n","                    self.epochs_per_file.append(num_valid)\n","                    epoch_data.append({'filepath': f_path, 'epoch_count': num_valid})\n","                except Exception as e:\n","                    file_name = Path(f_path).name\n","                    print(f\"Warning: Could not process {file_name}. Skipping. Error: {e}\")\n","                    self.epochs_per_file.append(0)\n","            print(\"âœ… One-time scan complete. Saving metadata file for future runs...\")\n","            metadata_df = pd.DataFrame(epoch_data)\n","            with self.fs.open(self.metadata_path, 'w') as f:\n","                metadata_df.to_csv(f, index=False)\n","            print(f\"âœ… Metadata saved to {self.metadata_path}.\")\n","\n","        self.cumulative_epochs = np.cumsum(self.epochs_per_file)\n","        self.total_epochs = self.cumulative_epochs[-1] if len(self.cumulative_epochs) > 0 else 0\n","        print(f\"âœ… Dataset initialized. Total valid epochs: {self.total_epochs}\")\n","\n","    def __len__(self):\n","        return self.total_epochs\n","\n","    def __getitem__(self, idx):\n","        file_idx = np.searchsorted(self.cumulative_epochs, idx, side='right')\n","        local_idx = idx - (self.cumulative_epochs[file_idx - 1] if file_idx > 0 else 0)\n","        file_path = self.file_paths[file_idx]\n","        if file_path not in self._cache:\n","            try:\n","                df = pd.read_parquet(file_path)\n","                self._cache[file_path] = df[df['label'].isin([0, 1, 2, 3, 4])].reset_index(drop=True)\n","            except Exception as e:\n","                raise IOError(f\"Error reading file {Path(file_path).name} in __getitem__: {e}\")\n","        row = self._cache[file_path].iloc[local_idx]\n","        label = np.int64(row['label'])\n","        spectrogram_flat = row.drop('label').values.astype(np.float32)\n","        mean, std = spectrogram_flat.mean(), spectrogram_flat.std()\n","        spectrogram_normalized = (spectrogram_flat - mean) / (std + 1e-6)\n","        spectrogram_2d = spectrogram_normalized.reshape(1, 76, 60)\n","        return torch.from_numpy(spectrogram_2d), torch.tensor(label)\n","\n","print(\"âœ… `OptimizedCombinedDataset` class defined.\")\n","\n","# ==============================================================================\n","# 7. TRAINING EXECUTION FOR FULL DATASET ON COLAB ENTERPRISE\n","# ==============================================================================\n","print(\"\\n--- Starting Full Dataset Training Run with Optimal Settings on GCS ---\")\n","\n","# --- General Parameters ---\n","# --- CHOOSE YOUR MODEL HERE ---\n","MODEL_TO_TRAIN = 'convnext_base' # Options: 'convnext_base', 'vit_base'\n","\n","EPOCHS = 40\n","BATCH_SIZE = 256\n","NUM_WORKERS = 8\n","CLASS_WEIGHTS = [0.7, 3.5, 0.5, 1.5, 1.2]\n","LEARNING_RATE = 5e-5\n","\n","# --- Paths and File Identification (using GCS) ---\n","GCS_BUCKET_PATH = \"gs://shhs-sleepedfx-data-bucket\"\n","METADATA_FILE_PATH = os.path.join(GCS_BUCKET_PATH, \"dataset_metadata.csv\")\n","\n","shhs1_processed_dir = f\"{GCS_BUCKET_PATH}/shhs1_processed\"\n","shhs2_processed_dir = f\"{GCS_BUCKET_PATH}/shhs2_processed\"\n","\n","# --- Load ALL files from GCS ---\n","print(\"Listing all files in GCS bucket...\")\n","fs = gcsfs.GCSFileSystem()\n","shhs1_files = fs.glob(f\"{shhs1_processed_dir}/*.parquet\")\n","shhs2_files = fs.glob(f\"{shhs2_processed_dir}/*.parquet\")\n","specific_shhs_file_paths = shhs1_files + shhs2_files\n","\n","# --- Main Training Logic ---\n","if not specific_shhs_file_paths:\n","     print(\"\\nERROR: No valid .parquet files were found in GCS. Aborting training.\")\n","else:\n","    print(f\"\\nFound {len(specific_shhs_file_paths)} total files for training.\")\n","\n","    full_dataset = OptimizedCombinedDataset(\n","        file_paths=specific_shhs_file_paths,\n","        metadata_path=METADATA_FILE_PATH\n","    )\n","\n","    if len(full_dataset) > 1:\n","        train_size = int(0.8 * len(full_dataset))\n","        val_size = len(full_dataset) - train_size\n","        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","        print(f\"Dataset split: {len(train_dataset)} training samples, {len(val_dataset)} validation samples.\")\n","\n","        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n","        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n","\n","        model = SleepStageClassifierLightning(\n","            model_name=MODEL_TO_TRAIN,\n","            learning_rate=LEARNING_RATE,\n","            class_weights=CLASS_WEIGHTS\n","        )\n","\n","        experiment_name = f\"{MODEL_TO_TRAIN}_full_dataset_gcs\"\n","        csv_logger = CSVLogger(f\"{GCS_BUCKET_PATH}/training_logs/\", name=experiment_name)\n","\n","        checkpoint_callback = ModelCheckpoint(\n","            monitor='val_loss',\n","            dirpath=f\"{GCS_BUCKET_PATH}/model_checkpoints/\",\n","            filename=f\"sleep-stage-model-{experiment_name}-{{epoch:02d}}-{{val_loss:.4f}}\",\n","            save_top_k=1,\n","            mode='min'\n","        )\n","\n","        early_stop_callback = EarlyStopping(\n","           monitor='val_loss',\n","           patience=7,\n","           verbose=True,\n","           mode='min'\n","        )\n","\n","        trainer = pl.Trainer(\n","            max_epochs=EPOCHS,\n","            accelerator=\"gpu\",\n","            devices=1,\n","            logger=csv_logger,\n","            callbacks=[checkpoint_callback, early_stop_callback],\n","            precision=\"bf16-mixed\",\n","            gradient_clip_val=1.0\n","        )\n","\n","        print(f\"\\nðŸš€ Starting model training for {MODEL_TO_TRAIN}...\")\n","        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","        print(f\"\\nâœ… Training complete!\")\n","        print(f\"Best model saved at: {checkpoint_callback.best_model_path}\")\n","\n","    else:\n","        print(\"Dataset is too small to split. Aborting training.\")"],"metadata":{"id":"kg-aUisgc2IA"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}