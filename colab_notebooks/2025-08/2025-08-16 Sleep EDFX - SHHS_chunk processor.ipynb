{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109707,"status":"ok","timestamp":1755366453855,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"},"user_tz":300},"id":"ZAmkt7vA2Ayo","outputId":"6a3fbc25-fd08-47d4-ad79-355b49f92ec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Instalando y actualizando librerías...\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h✅ Librerías listas.\n","\n","--- INICIANDO PRUEBA DE CONTROL DE GPU ---\n","✅ GPU detectada: NVIDIA A100-SXM4-40GB\n","✅ ¡Éxito! Memoria asignada activamente: 2.00 GB\n","✅ Memoria liberada correctamente.\n","--- PRUEBA DE CONTROL DE GPU COMPLETADA EXITOSAMENTE ---\n"]}],"source":["# Cell ID: ZAmkt7vA2Ayo\n","# Cell 1: This cell is essential for initial setup, connecting to Google Drive, installing libraries, and checking GPU availability.\n","# Step 1: Conectar con Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Instalar y actualizar las librerías\n","print(\"\\nInstalando y actualizando librerías...\")\n","!pip install --upgrade -q mne pytorch-lightning timm\n","print(\"✅ Librerías listas.\")\n","\n","# Step 3: Prueba explícita de control de la GPU\n","import torch\n","print(\"\\n--- INICIANDO PRUEBA DE CONTROL DE GPU ---\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"✅ GPU detectada: {torch.cuda.get_device_name(0)}\")\n","    try:\n","        tensor_grande = torch.randn(1024, 1024, 512, device=device) # Asignar 2GB\n","        memoria_asignada = torch.cuda.memory_allocated(0) / 1024**3\n","        print(f\"✅ ¡Éxito! Memoria asignada activamente: {memoria_asignada:.2f} GB\")\n","        del tensor_grande\n","        torch.cuda.empty_cache()\n","        print(\"✅ Memoria liberada correctamente.\")\n","        print(\"--- PRUEBA DE CONTROL DE GPU COMPLETADA EXITOSAMENTE ---\")\n","    except Exception as e:\n","        print(f\"❌ ¡ERROR DURANTE LA PRUEBA! No se pudo asignar memoria a la GPU: {e}\")\n","else:\n","    print(\"❌ ¡ERROR! No se detectó ninguna GPU en este entorno de ejecución.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12785,"status":"ok","timestamp":1755366148033,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"},"user_tz":300},"id":"qVOWMBr42MvW","outputId":"287a20f7-f318-43df-bd6d-fdcd01e3fb1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Funciones del modelo definidas exitosamente.\n"]}],"source":["# Cell ID: qVOWMBr42MvW\n","# Cell 2: This cell defines the Model Architecture (get_convnext_model).\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import timm\n","import pytorch_lightning as pl\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassCohenKappa\n","import numpy as np\n","\n","# ==============================================================================\n","# DEFINICIÓN DE LA ARQUITECTURA DEL MODELO\n","# ==============================================================================\n","def get_convnext_model(num_classes=5, pretrained=True):\n","    \"\"\"\n","    Crea un modelo ConvNeXT V2 adaptado para la clasificación de etapas del sueño.\n","    \"\"\"\n","    model = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=pretrained)\n","\n","    original_conv = model.stem[0]\n","    new_first_conv = nn.Conv2d(1, original_conv.out_channels,\n","                               kernel_size=original_conv.kernel_size,\n","                               stride=original_conv.stride,\n","                               padding=original_conv.padding,\n","                               bias=(original_conv.bias is not None))\n","\n","    with torch.no_grad():\n","        new_first_conv.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","\n","    model.stem[0] = new_first_conv\n","\n","    num_ftrs = model.head.fc.in_features\n","    model.head.fc = nn.Linear(num_ftrs, num_classes)\n","\n","    return model\n","\n","print(\"✅ Funciones del modelo definidas exitosamente.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1755366153383,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"},"user_tz":300},"id":"MAvU9zdM2SNw","outputId":"f1ce2330-9128-478d-fe24-dc4907eb947c"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ SleepStageClassifierLightning defined with ReduceLROnPlateau scheduler and direct gradient clipping in training_step_end.\n"]}],"source":["# Cell ID: MAvU9zdM2SNw OK\n","# Cell 3: This cell defines the PyTorch Lightning Module (SleepStageClassifierLightning), which is crucial for the training process.\n","import torch\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n","import numpy as np\n","import torch.optim as optim\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping # Removed GradientClipByNorm\n","from pytorch_lightning.loggers import CSVLogger\n","from torch.optim.lr_scheduler import ReduceLROnPlateau # Import the scheduler\n","from torch.nn.utils import clip_grad_norm_ # Import clip_grad_norm_\n","\n","# Reuse the get_convnext_model function from the previous cell\n","# from your_module import get_convnext_model # If get_convnext_model is in a separate file\n","\n","# ==============================================================================\n","# DEFINICIÓN DEL CLASIFICADOR USANDO PYTORCH LIGHTNING TRAINER\n","# ==============================================================================\n","class SleepStageClassifierLightning(pl.LightningModule):\n","    \"\"\"\n","    Módulo de PyTorch Lightning que encapsula nuestro modelo y la lógica de entrenamiento,\n","    diseñado para ser usado con el PyTorch Lightning Trainer.\n","    \"\"\"\n","    def __init__(self, model_name='convnextv2_tiny', num_classes=5, learning_rate=1e-4, class_weights=None):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = get_convnext_model(num_classes=num_classes, pretrained=True)\n","\n","        # Métricas\n","        self.train_accuracy = MulticlassAccuracy(num_classes=num_classes)\n","        self.train_f1 = MulticlassF1Score(num_classes=num_classes) # Use default 'macro' average or specify\n","        self.val_accuracy = MulticlassAccuracy(num_classes=num_classes)\n","        self.val_f1 = MulticlassF1Score(num_classes=num_classes) # Use default 'macro' average or specify\n","\n","        self.weights = torch.tensor(class_weights, dtype=torch.float) if class_weights is not None else None\n","        self.loss_fn = F.cross_entropy\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        # Data is automatically moved to the device by the Trainer\n","        y_pred_logits = self(x)\n","\n","        if self.weights is not None:\n","            loss = self.loss_fn(y_pred_logits, y_true, weight=self.weights.to(self.device))\n","        else:\n","            loss = self.loss_fn(y_pred_logits, y_true)\n","\n","        preds = torch.argmax(y_pred_logits, dim=1)\n","\n","        # Log metrics\n","        self.train_accuracy(preds, y_true)\n","        self.train_f1(y_pred_logits, y_true) # Pass logits and true labels to F1\n","        self.log('train_loss', loss)\n","        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=False, prog_bar=True)\n","        self.log('train_f1', self.train_f1, on_step=True, on_epoch=False)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        # Data is automatically moved to the device by the Trainer\n","\n","        # Add debugging prints and checks for NaN in input\n","        print(f\"Validation Step {batch_idx}:\")\n","        print(f\"  Input shape: {x.shape}, dtype: {x.dtype}\")\n","        print(f\"  True labels shape: {y_true.shape}, dtype: {y_true.dtype}\")\n","\n","        if torch.isnan(x).any():\n","            print(f\"  !!! WARNING: NaN values detected in input batch x in validation step {batch_idx} !!!\")\n","            # Optional: Print indices of samples with NaN inputs\n","            # nan_sample_indices = torch.where(torch.isnan(x).any(dim=[1, 2, 3]))[0]\n","            # print(f\"  Indices of samples with NaN input: {nan_sample_indices}\")\n","\n","        y_pred_logits = self(x)\n","\n","        # Add debugging print for predicted logits immediately after forward pass\n","        print(f\"  Predicted logits shape: {y_pred_logits.shape}, dtype: {y_pred_logits.dtype}\")\n","        print(f\"  Predicted logits (first 5): {y_pred_logits[:5]}\")\n","\n","        if torch.isnan(y_pred_logits).any():\n","             print(f\"  !!! WARNING: NaN values detected in predicted logits in validation step {batch_idx} !!!\")\n","             # Optional: Print indices of samples with NaN logits\n","             # nan_logit_indices = torch.where(torch.isnan(y_pred_logits).any(dim=1))[0]\n","             # print(f\"  Indices of samples with NaN logits: {nan_logit_indices}\")\n","\n","\n","        if self.weights is not None:\n","            # Ensure weights are on the correct device\n","            weights = self.weights.to(self.device)\n","            print(f\"  Using class weights: {weights}\")\n","            loss = self.loss_fn(y_pred_logits, y_true, weight=weights)\n","        else:\n","            loss = self.loss_fn(y_pred_logits, y_true)\n","\n","        # Add debugging print for loss\n","        print(f\"  Calculated loss: {loss.item()}\")\n","\n","        # Check for NaN loss\n","        if torch.isnan(loss):\n","             print(f\"  !!! WARNING: NaN loss detected in validation step {batch_idx} !!!\")\n","             # Optionally, you could try to identify which sample caused the NaN\n","             # This would require iterating through samples in the batch, which can be slow\n","             # For now, just logging the batch index and loss is a good start.\n","\n","\n","        preds = torch.argmax(y_pred_logits, dim=1)\n","\n","        # Log metrics\n","        self.val_accuracy(preds, y_true)\n","        self.val_f1(y_pred_logits, y_true) # Pass logits and true labels to F1\n","        self.log('val_loss', loss)\n","        self.log('val_acc', self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True)\n","\n","\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n","\n","        # Define the learning rate scheduler\n","        scheduler = {\n","            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True),\n","            'interval': 'epoch', # Step the scheduler after each epoch\n","            'frequency': 1,\n","            'monitor': 'val_loss' # Metric to monitor for reducing the learning rate\n","        }\n","\n","        return [optimizer], [scheduler] # Return optimizer and scheduler\n","\n","    # Add training_step_end to apply clipping\n","    def training_step_end(self, outputs):\n","         # Apply gradient clipping after gradients are computed but before optimizer step\n","         # This is suitable for automatic optimization\n","         clip_grad_norm_(self.parameters(), 1.0) # Corrected: Pass a numerical value for clipping\n","         return outputs\n","\n","\n","print(\"✅ SleepStageClassifierLightning defined with ReduceLROnPlateau scheduler and direct gradient clipping in training_step_end.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3900,"status":"ok","timestamp":1755366163167,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"},"user_tz":300},"id":"3Kaxttzn56u8","outputId":"add53767-695e-4722-cdbc-c1e3dc2a8c15"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Ensuring PyTorch Lightning and other libs are installed...\n","✅ Installation check complete.\n"]}],"source":["# Cell ID: 3Kaxttzn56u8\n","# Cell 4: Re-runs the library installation to ensure all necessary packages are available, especially if the runtime environment changes. It's good practice to keep this before the main training loop\n","# Re-run installation just in case\n","print(\"\\nEnsuring PyTorch Lightning and other libs are installed...\")\n","!pip install --upgrade -q pytorch-lightning timm\n","print(\"✅ Installation check complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1755366166812,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"},"user_tz":300},"id":"sqz0XnS36jjF","outputId":"e76d601e-7af6-44ab-d601-a6a3173e5480"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cell 4: Defining CombinedDataset class...\n","✅ CombinedDataset class defined with caching and improved error handling/logging.\n"]}],"source":["# Cell ID: sqz0XnS36jjF OK\n","# Cell 5: CombinedDataset Definition (Used for Chunked Training). This cell contains the CombinedDataset class, which is used by the training loop.\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pandas as pd\n","from pathlib import Path\n","import logging\n","import numpy as np\n","import time # Import time for optional profiling within getitem\n","\n","print(\"Cell 4: Defining CombinedDataset class...\")\n","\n","# --- Dataset that loads data from a list of file paths (for chunking) ---\n","class CombinedDataset(Dataset):\n","    def __init__(self, file_paths_chunk): # Modified to accept a list of file paths\n","        \"\"\"\n","        Initializes the dataset with a list of file paths for a specific chunk.\n","        \"\"\"\n","        print(f\"CombinedDataset: Initializing with {len(file_paths_chunk)} files.\")\n","        self.original_file_paths = file_paths_chunk\n","        self.file_paths = [] # This will store only the successfully processed file paths\n","\n","        logging.info(f\"Dataset chunk initialized with {len(self.original_file_paths)} subjects.\")\n","        print(\"CombinedDataset: Pre-calculating number of epochs per subject in chunk (this may take a moment)...\")\n","        logging.info(\"Pre-calculating the number of epochs per subject in chunk (this may take a moment)...\")\n","\n","        epochs_per_file_list = []\n","        successfully_processed_files = [] # To store file paths that were successfully processed\n","\n","        # Add the specific problematic file skip, use general error handling instead\n","        problematic_file = \"shhs2-200820.parquet\"\n","\n","        for i, f in enumerate(self.original_file_paths):\n","            # Adding more granular print with flush=True\n","            print(f\"CombinedDataset: Start processing file {i+1}/{len(self.original_file_paths)}: {f.name}\", flush=True)\n","\n","            if f.name == problematic_file:\n","                print(f\"CombinedDataset: Skipping problematic file: {f.name}\", flush=True)\n","                logging.warning(f\"Skipping problematic file: {f.name}\")\n","                epochs_per_file_list.append(0) # Append 0 epochs for the skipped file\n","                continue # Skip to the next file\n","\n","            try:\n","                df_labels = pd.read_parquet(f, columns=['label'])\n","                print(f\"CombinedDataset: Successfully read file {f.name}\", flush=True) # Added success print\n","                valid_labels = [0, 1, 2, 3, 4]\n","                num_valid_epochs = df_labels['label'].isin(valid_labels).sum()\n","\n","                # Only add files with valid epochs and successful reads to the processed list\n","                if num_valid_epochs > 0:\n","                    epochs_per_file_list.append(num_valid_epochs)\n","                    successfully_processed_files.append(f) # Add the file path here\n","                    print(f\"CombinedDataset: Found {num_valid_epochs} valid epochs in {f.name}\", flush=True) # Added valid epochs print\n","                else:\n","                    print(f\"CombinedDataset: No valid epochs found in {f.name}. Skipping file.\", flush=True)\n","                    logging.info(f\"No valid epochs found in {f.name}, skipping.\")\n","                    epochs_per_file_list.append(0)\n","\n","\n","            except Exception as e:\n","                print(f\"CombinedDataset: ERROR processing file {f.name}. Reason: {e}. Skipping file.\", flush=True) # More detailed error print\n","                logging.warning(f\"No se pudo leer o procesar el archivo {f.name}, se omitirá. Razón: {e}\")\n","                epochs_per_file_list.append(0) # Append 0 epochs for the errored file\n","\n","\n","        # Update self.file_paths with only the successfully processed files that had valid epochs\n","        self.file_paths = successfully_processed_files\n","\n","        # Recalculate cumulative epochs based on the successful files\n","        # Need to ensure epochs_per_file_list is aligned with successfully_processed_files\n","        # Let's rebuild epochs_per_file_list based on the successful files to be safe\n","        epochs_per_file_for_successful = []\n","        for f in self.file_paths:\n","            # Find the original index of this file to get its epoch count from the initial list\n","            original_index = self.original_file_paths.index(f)\n","            epochs_per_file_for_successful.append(epochs_per_file_list[original_index])\n","\n","        self.epochs_per_file = np.array(epochs_per_file_for_successful)\n","\n","\n","        self.cumulative_epochs = np.cumsum(self.epochs_per_file)\n","        self.total_epochs = self.cumulative_epochs[-1] if len(self.cumulative_epochs) > 0 else 0\n","\n","\n","        print(f\"CombinedDataset: Finished pre-calculation for chunk. Processed {len(self.file_paths)} files. Total valid epochs: {self.total_epochs}\")\n","        logging.info(f\"Número final de épocas válidas para el chunk: {self.total_epochs}\")\n","\n","        # --- Caching mechanism ---\n","        self._cache = {} # Dictionary to store cached dataframes\n","        print(\"CombinedDataset: Caching mechanism initialized.\")\n","\n","    def __len__(self):\n","        return self.total_epochs\n","\n","    def __getitem__(self, idx):\n","        # start_time_getitem = time.time() # Optional: start profiling\n","\n","        file_idx = np.searchsorted(self.cumulative_epochs, idx, side='right')\n","        file_path = self.file_paths[file_idx] # Use the list of successfully processed file paths\n","\n","        if file_idx == 0:\n","            local_idx_global = idx\n","        else:\n","            local_idx_global = idx - self.cumulative_epochs[file_idx - 1]\n","\n","        # --- Check cache first ---\n","        if file_path not in self._cache:\n","            # print(f\"__getitem__: Cache miss for {file_path.name}. Reading from disk.\", flush=True) # Optional print\n","            # Read the entire parquet file if not in cache\n","            try:\n","                df = pd.read_parquet(file_path)\n","                # Store in cache\n","                self._cache[file_path] = df\n","                # print(f\"__getitem__: Stored {file_path.name} in cache.\", flush=True) # Optional print\n","            except Exception as e:\n","                 print(f\"__getitem__: ERROR reading file {file_path.name}: {e}. Skipping sample.\", flush=True)\n","                 # Handle corrupted/unreadable files gracefully by returning None or raising an error\n","                 # Returning None might require collate_fn to handle None values. Raising error is simpler but stops batch.\n","                 # For now, let's re-raise the exception after logging\n","                 raise e # Re-raise the exception after logging\n","\n","\n","        # --- Get data from cache ---\n","        df_cached = self._cache[file_path]\n","\n","        # --- Process data from the cached dataframe ---\n","        valid_labels = [0, 1, 2, 3, 4]\n","        df_valid = df_cached[df_cached['label'].isin(valid_labels)]\n","\n","        # Ensure local_idx_global is within the bounds of the valid dataframe for this file\n","        # This check is crucial now as we only added files with valid epochs to self.file_paths\n","        if local_idx_global >= len(df_valid):\n","             print(f\"__getitem__: WARNING: Calculated local index {local_idx_global} is out of bounds for valid data in {file_path.name} (length {len(df_valid)}). This should not happen if pre-calculation is correct. Skipping sample.\", flush=True)\n","             # This indicates a potential issue in the epoch pre-calculation logic or data inconsistency\n","             raise IndexError(f\"Local index out of bounds for file {file_path.name}\")\n","\n","\n","        row = df_valid.iloc[local_idx_global]\n","        spectrogram_flat = row[:-1].values.astype(np.float32)\n","        label = np.int64(row['label'])\n","\n","        spectrogram_2d = spectrogram_flat.reshape(1, 76, 60)\n","\n","        # end_time_getitem = time.time() # Optional: end profiling\n","        # print(f\"__getitem__ for index {idx} from {file_path.name} took {end_time_getitem - start_time_getitem:.4f} seconds.\", flush=True) # Optional print\n","\n","        return torch.from_numpy(spectrogram_2d), torch.tensor(label)\n","\n","print(\"✅ CombinedDataset class defined with caching and improved error handling/logging.\")\n","\n","# NOTE: Data loading and Trainer setup logic is moved to the chunked training loop (Cell 6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1ruaWyKQ7e6t","executionInfo":{"status":"error","timestamp":1755366229766,"user_tz":300,"elapsed":54466,"user":{"displayName":"Daniel Hinostroza","userId":"01416331706775365219"}},"outputId":"61abb1c2-faac-43c5-d6b9-288f533868e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cell 6: Starting chunked training loop execution...\n","Cell 6: Defining paths and checking for staged data...\n","Cell 6: Checkpoint directory: /content/drive/MyDrive/final_model_checkpoint\n","Cell 6: Discovering and chunking files from local staged data...\n","\n","WARNING: Local staged data not found. Please run Cell 4 (Data Staging).\n","Attempting to discover files directly from Drive instead (will be slower)...\n","Cell 6: Found 8628 total files.\n","chunk_size not found from Cell 5, using default: 50\n","Cell 6: Dividing 8628 files into chunks of size 50...\n","Cell 6: Divided into 173 chunks.\n","Cell 6: Starting training loop over 173 chunks.\n","\n","--- Processing Chunk 126/173 ---\n","Number of files in current chunk: 50\n","Initializing CombinedDataset for chunk 126...\n","CombinedDataset: Initializing with 50 files.\n","CombinedDataset: Pre-calculating number of epochs per subject in chunk (this may take a moment)...\n","CombinedDataset: Start processing file 1/50: shhs2-200771.parquet\n","CombinedDataset: Successfully read file shhs2-200771.parquet\n","CombinedDataset: Found 1181 valid epochs in shhs2-200771.parquet\n","CombinedDataset: Start processing file 2/50: shhs2-200775.parquet\n","CombinedDataset: Successfully read file shhs2-200775.parquet\n","CombinedDataset: Found 1500 valid epochs in shhs2-200775.parquet\n","CombinedDataset: Start processing file 3/50: shhs2-200778.parquet\n","CombinedDataset: Successfully read file shhs2-200778.parquet\n","CombinedDataset: Found 1205 valid epochs in shhs2-200778.parquet\n","CombinedDataset: Start processing file 4/50: shhs2-200780.parquet\n","CombinedDataset: Successfully read file shhs2-200780.parquet\n","CombinedDataset: Found 1377 valid epochs in shhs2-200780.parquet\n","CombinedDataset: Start processing file 5/50: shhs2-200782.parquet\n","CombinedDataset: Successfully read file shhs2-200782.parquet\n","CombinedDataset: Found 1077 valid epochs in shhs2-200782.parquet\n","CombinedDataset: Start processing file 6/50: shhs2-200784.parquet\n","CombinedDataset: Successfully read file shhs2-200784.parquet\n","CombinedDataset: Found 1232 valid epochs in shhs2-200784.parquet\n","CombinedDataset: Start processing file 7/50: shhs2-200785.parquet\n","CombinedDataset: Successfully read file shhs2-200785.parquet\n","CombinedDataset: Found 1121 valid epochs in shhs2-200785.parquet\n","CombinedDataset: Start processing file 8/50: shhs2-200786.parquet\n","CombinedDataset: Successfully read file shhs2-200786.parquet\n","CombinedDataset: Found 1116 valid epochs in shhs2-200786.parquet\n","CombinedDataset: Start processing file 9/50: shhs2-200787.parquet\n","CombinedDataset: Successfully read file shhs2-200787.parquet\n","CombinedDataset: Found 1428 valid epochs in shhs2-200787.parquet\n","CombinedDataset: Start processing file 10/50: shhs2-200789.parquet\n","CombinedDataset: Successfully read file shhs2-200789.parquet\n","CombinedDataset: Found 1369 valid epochs in shhs2-200789.parquet\n","CombinedDataset: Start processing file 11/50: shhs2-200790.parquet\n","CombinedDataset: Successfully read file shhs2-200790.parquet\n","CombinedDataset: Found 1454 valid epochs in shhs2-200790.parquet\n","CombinedDataset: Start processing file 12/50: shhs2-200793.parquet\n","CombinedDataset: Successfully read file shhs2-200793.parquet\n","CombinedDataset: Found 1320 valid epochs in shhs2-200793.parquet\n","CombinedDataset: Start processing file 13/50: shhs2-200795.parquet\n","CombinedDataset: Successfully read file shhs2-200795.parquet\n","CombinedDataset: Found 1433 valid epochs in shhs2-200795.parquet\n","CombinedDataset: Start processing file 14/50: shhs2-200796.parquet\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-702446598.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initializing CombinedDataset for chunk {chunk_idx + 1}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Assuming CombinedDataset is defined in Cell 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mchunk_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths_chunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass the list of files for this chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total valid epochs in chunk {chunk_idx + 1}: {len(chunk_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1943002438.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_paths_chunk)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mdf_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CombinedDataset: Successfully read file {f.name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Added success print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             pa_table = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         dataset = ParquetDataset(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_fragment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             self._dataset = ds.FileSystemDataset(\n\u001b[0m\u001b[1;32m   1360\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Cell ID: 1ruaWyKQ7e6t\n","# Cell 6: This cell contains the complete logic for execution of the chunked training loop, including data discovery,\n","# chunking, model loading/initialization, trainer setup, and the training process with checkpointing.\n","# Start the loop from the chunk where the previous session dropped off (chunk_idx = 124)\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import CSVLogger\n","import numpy as np\n","import os\n","import glob\n","import pandas as pd # Ensure pandas is imported\n","from pathlib import Path # Ensure Path is imported\n","\n","# Set matrix multiplication precision for Tensor Cores\n","torch.set_float32_matmul_precision('medium')\n","\n","print(\"Cell 6: Starting chunked training loop execution...\")\n","\n","# --- Define training parameters ---\n","epochs_per_chunk = 5 # Train for 5 epochs on each chunk (adjust as needed)\n","batch_size = 256\n","num_workers = 4 # Use more workers for potentially faster loading from local disk\n","class_weights = [0.7, 3.5, 0.5, 1.5, 1.2] # Reuse weights\n","learning_rate = 1e-4\n","\n","# --- Rutas y Configuración ---\n","print(\"Cell 6: Defining paths and checking for staged data...\")\n","sleep_edfx_processed_dir_local = Path('/content/processed_data/sleep_edfx_processed/')\n","shhs1_processed_dir_local = Path('/content/processed_data/shhs1_processed/')\n","shhs2_processed_dir_local = Path('/content/processed_data/shhs2_processed/')\n","final_checkpoint_dir = Path('/content/drive/MyDrive/final_model_checkpoint/') # Checkpoints still saved to Drive\n","final_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n","print(f\"Cell 6: Checkpoint directory: {final_checkpoint_dir}\")\n","\n","# --- Discover and chunk files (from local staged data) ---\n","# Assuming data was staged to /content/processed_data/ in Cell 4\n","print(\"Cell 6: Discovering and chunking files from local staged data...\")\n","# Check if the local staged directory exists and has data\n","if not sleep_edfx_processed_dir_local.exists() or not any(sleep_edfx_processed_dir_local.iterdir()):\n","    print(\"\\nWARNING: Local staged data not found. Please run Cell 4 (Data Staging).\")\n","    print(\"Attempting to discover files directly from Drive instead (will be slower)...\")\n","    sleep_edfx_processed_dir_source = Path('/content/drive/MyDrive/sleep_edfx_processed/')\n","    shhs1_processed_dir_source = Path('/content/drive/MyDrive/shhs1_processed/')\n","    shhs2_processed_dir_source = Path('/content/drive/MyDrive/shhs2_processed/')\n","else:\n","    print(\"Cell 6: Using local staged data.\")\n","    sleep_edfx_processed_dir_source = sleep_edfx_processed_dir_local\n","    shhs1_processed_dir_source = shhs1_processed_dir_local\n","    shhs2_processed_dir_source = shhs2_processed_dir_local\n","\n","\n","edfx_files = sorted(list(sleep_edfx_processed_dir_source.glob('*.parquet')))\n","shhs1_files = sorted(list(shhs1_processed_dir_source.glob('**/*.parquet')))\n","shhs2_files = sorted(list(shhs2_processed_dir_source.glob('**/*.parquet')))\n","all_files = edfx_files + shhs1_files + shhs2_files\n","\n","if not all_files:\n","    print(\"\\nERROR: No data files found in the specified directories (local staged or Drive). Cannot proceed with training.\")\n","else:\n","    print(f\"Cell 6: Found {len(all_files)} total files.\")\n","\n","    # Define chunk size (e.g., number of files per chunk) - Reuse from Cell 5 or define here\n","    # Assuming chunk_size is defined in Cell 5 and available, or define a default:\n","    try:\n","        if 'chunk_size' not in locals() and 'chunk_size' not in globals():\n","            chunk_size = 50 # Default chunk size if not defined in Cell 5\n","            print(f\"chunk_size not found from Cell 5, using default: {chunk_size}\")\n","    except NameError:\n","         chunk_size = 50 # Default chunk size\n","         print(f\"chunk_size not found, using default: {chunk_size}\")\n","\n","\n","    print(f\"Cell 6: Dividing {len(all_files)} files into chunks of size {chunk_size}...\")\n","    file_chunks = [all_files[i:i + chunk_size] for i in range(0, len(all_files), chunk_size)]\n","    print(f\"Cell 6: Divided into {len(file_chunks)} chunks.\")\n","\n","\n","    # Define logger (Instantiate once for the entire run)\n","    logger = CSVLogger(\"logs\", name=\"sleep_stage_training\")\n","\n","    # --- Outer loop through data chunks ---\n","    print(f\"Cell 6: Starting training loop over {len(file_chunks)} chunks.\")\n","\n","    # Variable to keep track of the model for loading checkpoints\n","    model_lightning = None\n","    latest_checkpoint_path = None # Track the path of the latest checkpoint to load\n","\n","    # Start the loop from the chunk where the previous session dropped off (chunk_idx = 0)\n","    for chunk_idx in range(0, len(file_chunks)):\n","        file_paths_chunk = file_chunks[chunk_idx]\n","        print(f\"\\n--- Processing Chunk {chunk_idx + 1}/{len(file_chunks)} ---\")\n","        print(f\"Number of files in current chunk: {len(file_paths_chunk)}\")\n","\n","        if not file_paths_chunk:\n","            print(f\"Chunk {chunk_idx + 1} is empty. Skipping.\")\n","            continue\n","\n","        # a. Create a CombinedDataset instance for the current chunk\n","        print(f\"Initializing CombinedDataset for chunk {chunk_idx + 1}...\")\n","        # Assuming CombinedDataset is defined in Cell 4\n","        chunk_dataset = CombinedDataset(file_paths_chunk) # Pass the list of files for this chunk\n","        print(f\"Total valid epochs in chunk {chunk_idx + 1}: {len(chunk_dataset)}\")\n","\n","\n","        if len(chunk_dataset) == 0:\n","            print(f\"Chunk {chunk_idx + 1} contains no valid epochs. Skipping.\")\n","            continue\n","\n","        # b. Split the current chunk's dataset into train and validation\n","        print(f\"Splitting dataset for chunk {chunk_idx + 1}...\")\n","        # Ensure sufficient data for split\n","        if len(chunk_dataset) < 2: # Need at least 2 samples to split\n","             print(f\"Chunk {chunk_idx + 1} has only {len(chunk_dataset)} valid epochs. Cannot split. Skipping.\")\n","             continue\n","\n","        train_size = int(0.8 * len(chunk_dataset))\n","        val_size = len(chunk_dataset) - train_size\n","\n","        # Adjust sizes if train_size or val_size is zero after calculation\n","        if train_size == 0:\n","            print(f\"Train size is 0 for chunk {chunk_idx + 1}. Cannot split. Skipping.\")\n","            continue\n","        # If val_size is 0 but total dataset is > 0, proceed with training only\n","        if val_size == 0:\n","             print(f\"Validation size is 0 for chunk {chunk_idx + 1}. Using entire chunk for training.\")\n","             train_size = len(chunk_dataset)\n","             val_dataset = None\n","             train_dataset = chunk_dataset\n","        else:\n","            train_dataset, val_dataset = random_split(chunk_dataset, [train_size, val_size])\n","\n","\n","        print(f\"Chunk {chunk_idx + 1}: Train size: {len(train_dataset)}, Val size: {len(val_dataset) if val_dataset else 0}\")\n","\n","\n","        # c. Create DataLoaders for the current chunk\n","        print(f\"Creating DataLoaders for chunk {chunk_idx + 1}...\")\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, persistent_workers=True) if val_dataset else None\n","        print(f\"DataLoaders created for chunk {chunk_idx + 1}.\")\n","\n","        # d. Initialize or load the SleepStageClassifierLightning model\n","        # Always attempt to load the latest checkpoint from the previous chunk if it exists\n","        print(f\"Attempting to find and load latest checkpoint from {final_checkpoint_dir}...\")\n","\n","        # Prioritize the explicitly saved 'latest_model_chunk_{prev_chunk_idx+1}.ckpt'\n","        # The previous chunk index for explicit save filename is chunk_idx (since loop is 0-indexed)\n","        prev_chunk_checkpoint_explicit = final_checkpoint_dir / f'latest_model_chunk_{chunk_idx}.ckpt'\n","\n","        if prev_chunk_checkpoint_explicit.exists():\n","            latest_checkpoint_path = str(prev_chunk_checkpoint_explicit)\n","            print(f\"Found latest explicit checkpoint from previous chunk: {latest_checkpoint_path}\")\n","        else:\n","             # Fallback to finding the latest ModelCheckpoint saved during the previous chunk's training\n","             # This looks for files matching the pattern 'lightning-chunk-{prev_chunk_idx}-*.ckpt'\n","             # The previous chunk index for the filename pattern is chunk_idx (0-indexed) + 1 for the chunk number in filename\n","             prev_chunk_filename_pattern = f'lightning-chunk-{chunk_idx}-*.ckpt'\n","             checkpoint_files_prev_chunk = sorted(glob.glob(str(final_checkpoint_dir / prev_chunk_filename_pattern)))\n","\n","\n","             if checkpoint_files_prev_chunk:\n","                  latest_checkpoint_path = checkpoint_files_prev_chunk[-1] # Take the last one alphabetically/chronologically\n","                  print(f\"Found latest ModelCheckpoint from previous chunk: {latest_checkpoint_path}\")\n","             else:\n","                  latest_checkpoint_path = None\n","                  print(f\"No specific or ModelCheckpoint found for chunk {chunk_idx}.\")\n","\n","\n","        if latest_checkpoint_path:\n","            print(f\"Loading model from checkpoint: {latest_checkpoint_path}\")\n","            # Load the checkpoint\n","            try:\n","                # Use load_from_checkpoint for LightningModule, passing required hyperparameters\n","                # Assuming SleepStageClassifierLightning is defined in Cell 3\n","                 model_lightning = SleepStageClassifierLightning.load_from_checkpoint(\n","                     latest_checkpoint_path,\n","                     learning_rate=learning_rate, # Pass hyperparameters if needed by __init__\n","                     class_weights=class_weights\n","                 )\n","                 print(\"Model loaded successfully from checkpoint.\")\n","            except Exception as e:\n","                 print(f\"Error loading model from checkpoint {latest_checkpoint_path}: {e}\")\n","                 print(\"Initializing a new model instead for this chunk.\")\n","                 # Assuming SleepStageClassifierLightning is defined in Cell 3\n","                 model_lightning = SleepStageClassifierLightning(learning_rate=learning_rate, class_weights=class_weights)\n","                 latest_checkpoint_path = None # Reset checkpoint path if loading failed\n","        else:\n","            # If no checkpoint found (e.g., first chunk or loading failed), initialize a new model\n","            print(\"No checkpoint found from previous chunk. Initializing a new model.\")\n","            # Assuming SleepStageClassifierLightning is defined in Cell 3\n","            model_lightning = SleepStageClassifierLightning(learning_rate=learning_rate, class_weights=class_weights)\n","            # latest_checkpoint_path remains None\n","\n","\n","        # f. Define ModelCheckpoint callback for the current chunk\n","        # Filename now includes chunk index for distinct checkpoints within each chunk's training run\n","        # Use chunk_idx + 1 for the filename to match human-readable chunk numbering\n","        checkpoint_callback = ModelCheckpoint(\n","            dirpath=final_checkpoint_dir,\n","            filename=f'lightning-chunk-{chunk_idx + 1}-{{epoch:02d}}-{{val_loss:.4f}}',\n","            monitor='val_loss', # Monitor validation loss\n","            mode='min',\n","            save_top_k=1, # Save the best model based on the monitor for this chunk's training\n","            save_last=True # Also save the model from the last epoch of this chunk's training\n","        )\n","        print(f\"ModelCheckpoint callback defined for chunk {chunk_idx + 1}.\")\n","\n","        # g. Initialize the PyTorch Lightning Trainer for the current chunk\n","        # The Trainer needs to be re-initialized for each chunk to handle the new dataloaders and potentially loaded model state\n","        print(f\"Initializing Trainer for chunk {chunk_idx + 1}...\")\n","        trainer = pl.Trainer(\n","            max_epochs=epochs_per_chunk,\n","            accelerator=\"gpu\",\n","            devices=1, # Assuming you want to use one GPU\n","            callbacks=[checkpoint_callback], # Add early_stop_callback here if used\n","            logger=logger, # Use the same logger instance for all chunks\n","            precision=\"32\", # Enable mixed precision\n","            accumulate_grad_batches=4,\n","            # Add other trainer arguments as needed, e.g., limit_train_batches, limit_val_batches\n","            # limit_train_batches=0.1, # Example: use only 10% of training data per chunk\n","            # limit_val_batches=0.1 # Example: use only 10% of validation data per chunk\n","        )\n","        print(f\"Trainer initialized for chunk {chunk_idx + 1}.\")\n","\n","        # h. Call trainer.fit() to train on the current chunk\n","        print(f\"Starting training for chunk {chunk_idx + 1}...\")\n","        # Pass the model and data loaders to the trainer\n","        if val_loader is not None:\n","            trainer.fit(model_lightning, train_loader, val_loader)\n","        else:\n","            # If no validation data, train without validation loader\n","            trainer.fit(model_lightning, train_loader)\n","\n","        print(f\"Training completed for chunk {chunk_idx + 1}.\")\n","\n","        # i. Explicitly save the model state after training on the chunk\n","        # Save to a fixed name with chunk index so it's easy to find the latest for the next iteration\n","        final_chunk_checkpoint_path = final_checkpoint_dir / f'latest_model_chunk_{chunk_idx + 1}.ckpt'\n","        print(f\"Saving final model state for chunk {chunk_idx + 1} to {final_chunk_checkpoint_path}...\")\n","        trainer.save_checkpoint(final_chunk_checkpoint_path)\n","        print(f\"Model state saved for chunk {chunk_idx + 1}.\")\n","\n","        # latest_checkpoint_path is already updated by the explicit save\n","\n","\n","    print(\"\\n========== CHUNKED TRAINING LOOP COMPLETED ==========\")"]},{"cell_type":"code","metadata":{"id":"16c810ee"},"source":["# Cell ID: 16c810ee\n","# Cell 7: Finding/display the latest log file and create a backup copy of it.\n","import pandas as pd\n","import glob\n","import os\n","import shutil # Import shutil for file copying\n","from pathlib import Path # Import Path\n","\n","# Define the base directory for logs on Google Drive (where CSVLogger is configured to save)\n","log_dir_drive = '/content/drive/MyDrive/sleep_logs/sleep_stage_training/' # Updated to the expected logger path\n","\n","# --- Find the latest version directory and the metrics.csv file ---\n","version_dirs = sorted(glob.glob(os.path.join(log_dir_drive, 'version_*')))\n","\n","log_file_path = None\n","latest_version_dir = None\n","\n","if not version_dirs:\n","    print(f\"No version directories found in {log_dir_drive}. Please ensure training has started and logs are being generated.\")\n","else:\n","    latest_version_dir = version_dirs[-1]\n","    print(f\"Found latest log version directory: {latest_version_dir}\")\n","\n","    # Look for the metrics.csv file within the latest version directory\n","    metrics_file_path_candidate = os.path.join(latest_version_dir, 'metrics.csv') # CSVLogger default filename\n","\n","    if os.path.exists(metrics_file_path_candidate):\n","        log_file_path = metrics_file_path_candidate\n","        print(f\"Found log file: {log_file_path}\")\n","    else:\n","        print(f\"Could not find 'metrics.csv' in {latest_version_dir}. Please check the exact filename in your log directory.\")\n","        # Fallback or additional checks could be added here if the filename might vary\n","\n","# --- Perform backup and display metrics if log file was found ---\n","if log_file_path:\n","    # --- Define the path for the backup copy ---\n","    # You can choose a different name or directory for the backup\n","    backup_log_file_path = \"/content/drive/MyDrive/sleep_logs/metrics_backup_for_analysis.csv\" # Example backup path - UPDATE THIS if needed\n","\n","    # Create backup copy\n","    try:\n","        shutil.copy2(log_file_path, backup_log_file_path)\n","        print(f\"\\nSuccessfully created backup of metrics.csv at: {backup_log_file_path}\")\n","    except Exception as e:\n","        print(f\"\\nError creating backup copy: {e}\")\n","\n","\n","    # --- Load and display metrics ---\n","    try:\n","        log_df = pd.read_csv(log_file_path)\n","\n","        # Display relevant columns\n","        print(\"\\nValidation Metrics from Logs:\")\n","        if 'val_acc' in log_df.columns and 'val_f1' in log_df.columns:\n","             # Only show columns relevant to epoch-end validation metrics\n","             # These typically have non-NaN values only at the end of an epoch\n","             validation_metrics_df = log_df[['epoch', 'val_loss', 'val_acc', 'val_f1']].dropna(subset=['val_loss', 'val_acc', 'val_f1'])\n","             display(validation_metrics_df)\n","        else:\n","             print(\"Could not find 'val_acc' or 'val_f1' columns in the log file.\")\n","             print(\"Available columns:\", log_df.columns.tolist())\n","\n","\n","    except Exception as e:\n","        print(f\"Error loading or processing log file {log_file_path}: {e}\")\n","\n","else:\n","    print(\"\\nLog file not found. Cannot perform backup or display metrics.\")"],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1SGccv9QUvfIuFXjNWfQ5kaNj6IjaweQY","timestamp":1755367429040}],"authorship_tag":"ABX9TyNBPTKTXxpcA8PbmHB6IBDr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}