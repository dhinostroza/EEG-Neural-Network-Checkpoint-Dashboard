{"cells":[{"cell_type":"code","source":["# Cell 1: Initial setup, connecting to Google Drive, installing libraries, and checking GPU availability.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Instalar y actualizar las librerÃ­as\n","print(\"\\nInstalando y actualizando librerÃ­as...\")\n","!pip install --upgrade -q mne pytorch-lightning timm\n","print(\"âœ… LibrerÃ­as listas.\")\n","\n","# Step 3: Prueba explÃ­cita de control de la GPU\n","import torch\n","print(\"\\n--- INICIANDO PRUEBA DE CONTROL DE GPU ---\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"âœ… GPU detectada: {torch.cuda.get_device_name(0)}\")\n","    try:\n","        tensor_grande = torch.randn(1024, 1024, 512, device=device) # Asignar 2GB\n","        memoria_asignada = torch.cuda.memory_allocated(0) / 1024**3\n","        print(f\"âœ… Â¡Ã‰xito! Memoria asignada activamente: {memoria_asignada:.2f} GB\")\n","        del tensor_grande\n","        torch.cuda.empty_cache()\n","        print(\"âœ… Memoria liberada correctamente.\")\n","        print(\"--- PRUEBA DE CONTROL DE GPU COMPLETADA EXITOSAMENTE ---\")\n","    except Exception as e:\n","        print(f\"âŒ Â¡ERROR DURANTE LA PRUEBA! No se pudo asignar memoria a la GPU: {e}\")\n","else:\n","    print(\"âŒ Â¡ERROR! No se detectÃ³ ninguna GPU en este entorno de ejecuciÃ³n.\")"],"metadata":{"id":"2joTy7U_sars"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# 1. SETUP AND DEPENDENCY INSTALLATION\n","# ==============================================================================\n","print(\"Ensuring PyTorch Lightning and other libraries are installed...\")\n","# Install the necessary libraries with pinned versions to avoid conflicts\n","!pip install --upgrade -q pytorch-lightning timm \"pandas==2.2.2\" \"pyarrow==19.0.0\"\n","print(\"âœ… Installation check complete.\")\n","\n","# ==============================================================================\n","# 2. IMPORTS AND INITIAL CONFIGURATION\n","# ==============================================================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import timm\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import CSVLogger\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import os\n","\n","# Set matrix multiplication precision for A100/H100 GPUs for better performance\n","torch.set_float32_matmul_precision('medium')\n","print(\"âœ… Libraries imported and configuration set.\")\n","\n","# ==============================================================================\n","# 3. MODEL ARCHITECTURE DEFINITION (ADVANCED MODELS)\n","# ==============================================================================\n","def get_model(model_name='convnext_tiny', num_classes=5, pretrained=True):\n","    \"\"\"\n","    Creates a model adapted for sleep stage classification.\n","    Supports ConvNeXT, Swin Transformer, and MaxViT architectures.\n","    \"\"\"\n","    if model_name == 'convnext_tiny':\n","        model = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=pretrained)\n","        original_conv = model.stem[0]\n","        new_first_conv = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","        with torch.no_grad():\n","            if original_conv.weight.shape[1] == 3:\n","                new_first_conv.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","        model.stem[0] = new_first_conv\n","        num_ftrs = model.head.fc.in_features\n","        model.head.fc = nn.Linear(num_ftrs, num_classes)\n","        print(f\"âœ… ConvNeXT Tiny model created.\")\n","\n","    elif model_name == 'swin_base':\n","        model = timm.create_model('swin_base_patch4_window7_224.ms_in22k', pretrained=pretrained, img_size=(76, 60))\n","        original_conv = model.patch_embed.proj\n","        new_patch_embed = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","        with torch.no_grad():\n","            if original_conv.weight.shape[1] == 3:\n","                new_patch_embed.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","        model.patch_embed.proj = new_patch_embed\n","        num_ftrs = model.head.in_features\n","        model.head = nn.Linear(num_ftrs, num_classes)\n","        print(f\"âœ… Swin Transformer Base model created.\")\n","\n","    elif model_name == 'maxvit_base':\n","        model = timm.create_model('maxvit_base_tf_224.in1k', pretrained=pretrained, img_size=(76, 60))\n","        original_conv = model.stem.conv1\n","        new_first_conv = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","        with torch.no_grad():\n","            if original_conv.weight.shape[1] == 3:\n","                new_first_conv.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","        model.stem.conv1 = new_first_conv\n","        num_ftrs = model.head.fc.in_features\n","        model.head.fc = nn.Linear(num_ftrs, num_classes)\n","        print(f\"âœ… MaxViT Base model created.\")\n","\n","    else:\n","        raise ValueError(f\"Model '{model_name}' not supported.\")\n","\n","    return model\n","\n","print(\"âœ… `get_model` function defined with advanced architecture support.\")\n","\n","# ==============================================================================\n","# 4. PYTORCH LIGHTNING MODULE\n","# ==============================================================================\n","class SleepStageClassifierLightning(pl.LightningModule):\n","    def __init__(self, model_name, learning_rate=1e-5, class_weights=None):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = get_model(model_name=self.hparams.model_name, num_classes=5, pretrained=True)\n","        self.train_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.val_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.weights = torch.tensor(class_weights, dtype=torch.float) if class_weights is not None else None\n","        self.loss_fn = nn.CrossEntropyLoss(weight=self.weights)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        y_pred_logits = self(x)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_acc', self.train_accuracy(y_pred_logits, y_true), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        y_pred_logits = self(x)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', self.val_accuracy(y_pred_logits, y_true), on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n","        scheduler = {\n","            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3),\n","            'monitor': 'val_loss',\n","            'interval': 'epoch',\n","            'frequency': 1,\n","        }\n","        return [optimizer], [scheduler]\n","\n","print(\"âœ… `SleepStageClassifierLightning` module defined.\")\n","\n","# ==============================================================================\n","# 5. CUSTOM DATASET DEFINITION\n","# ==============================================================================\n","class CombinedDataset(Dataset):\n","    def __init__(self, file_paths_chunk):\n","        print(f\"Initializing dataset with {len(file_paths_chunk)} files...\")\n","        self.file_paths = file_paths_chunk\n","        self.epochs_per_file = []\n","        self._cache = {}\n","        for f_path in self.file_paths:\n","            try:\n","                df_labels = pd.read_parquet(f_path, columns=['label'])\n","                num_valid = df_labels['label'].isin([0, 1, 2, 3, 4]).sum()\n","                self.epochs_per_file.append(num_valid)\n","            except Exception as e:\n","                print(f\"Warning: Could not read or process {f_path.name}. Skipping. Error: {e}\")\n","                self.epochs_per_file.append(0)\n","        self.cumulative_epochs = np.cumsum(self.epochs_per_file)\n","        self.total_epochs = self.cumulative_epochs[-1] if len(self.cumulative_epochs) > 0 else 0\n","        print(f\"âœ… Dataset initialized. Total valid epochs: {self.total_epochs}\")\n","\n","    def __len__(self):\n","        return self.total_epochs\n","\n","    def __getitem__(self, idx):\n","        file_idx = np.searchsorted(self.cumulative_epochs, idx, side='right')\n","        local_idx = idx - (self.cumulative_epochs[file_idx - 1] if file_idx > 0 else 0)\n","        file_path = self.file_paths[file_idx]\n","        if file_path not in self._cache:\n","            try:\n","                df = pd.read_parquet(file_path)\n","                self._cache[file_path] = df[df['label'].isin([0, 1, 2, 3, 4])].reset_index(drop=True)\n","            except Exception as e:\n","                raise IOError(f\"Error reading file {file_path.name} in __getitem__: {e}\")\n","        row = self._cache[file_path].iloc[local_idx]\n","        label = np.int64(row['label'])\n","        spectrogram_flat = row.drop('label').values.astype(np.float32)\n","        mean, std = spectrogram_flat.mean(), spectrogram_flat.std()\n","        spectrogram_normalized = (spectrogram_flat - mean) / (std + 1e-6)\n","        spectrogram_2d = spectrogram_normalized.reshape(1, 76, 60)\n","        return torch.from_numpy(spectrogram_2d), torch.tensor(label)\n","\n","print(\"âœ… `CombinedDataset` class defined.\")\n","\n","# ==============================================================================\n","# 6. TRAINING EXECUTION\n","# ==============================================================================\n","print(\"\\n--- Starting Advanced Model Comparison Experiment ---\")\n","\n","# --- General Parameters ---\n","MODELS_TO_TEST = ['convnext_tiny', 'swin_base', 'maxvit_base']\n","EPOCHS = 40\n","BATCH_SIZE = 256\n","NUM_WORKERS = 8\n","CLASS_WEIGHTS = [0.7, 3.5, 0.5, 1.5, 1.2]\n","LEARNING_RATE = 5e-5\n","\n","# --- Paths and File Identification (using Google Drive) ---\n","shhs1_processed_dir_base = Path('/content/drive/MyDrive/shhs1_processed')\n","shhs2_processed_dir_base = Path('/content/drive/MyDrive/shhs2_processed')\n","\n","shhs1_files = list(shhs1_processed_dir_base.glob('*.parquet'))[:50]\n","shhs2_files = list(shhs2_processed_dir_base.glob('*.parquet'))[:50]\n","specific_shhs_file_paths = shhs1_files + shhs2_files\n","\n","# --- Main Experiment Loop ---\n","if not specific_shhs_file_paths:\n","     print(\"\\nERROR: No valid .parquet files were found. Aborting experiment.\")\n","else:\n","    print(f\"\\nFound {len(specific_shhs_file_paths)} specific files for training.\")\n","\n","    full_dataset = CombinedDataset(specific_shhs_file_paths)\n","\n","    if len(full_dataset) > 1:\n","        train_size = int(0.8 * len(full_dataset))\n","        val_size = len(full_dataset) - train_size\n","        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","        print(f\"Dataset split: {len(train_dataset)} training samples, {len(val_dataset)} validation samples.\")\n","\n","        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n","        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n","\n","        for model_name in MODELS_TO_TEST:\n","            print(f\"\\n{'='*20} STARTING EXPERIMENT FOR MODEL: {model_name.upper()} {'='*20}\")\n","\n","            model = SleepStageClassifierLightning(\n","                model_name=model_name,\n","                learning_rate=LEARNING_RATE,\n","                class_weights=CLASS_WEIGHTS\n","            )\n","\n","            experiment_name = f\"{model_name}_100_files_advanced_comparison\"\n","            csv_logger = CSVLogger(\"/content/drive/MyDrive/sleep_logs/\", name=experiment_name)\n","\n","            checkpoint_callback = ModelCheckpoint(\n","                monitor='val_loss',\n","                dirpath='/content/drive/MyDrive/final_model_checkpoint/',\n","                filename=f\"sleep-stage-model-{experiment_name}-{{epoch:02d}}-{{val_loss:.4f}}\",\n","                save_top_k=1,\n","                mode='min'\n","            )\n","\n","            early_stop_callback = EarlyStopping(\n","               monitor='val_loss',\n","               patience=7,\n","               verbose=True,\n","               mode='min'\n","            )\n","\n","            trainer = pl.Trainer(\n","                max_epochs=EPOCHS,\n","                accelerator=\"gpu\",\n","                devices=1,\n","                logger=csv_logger,\n","                callbacks=[checkpoint_callback, early_stop_callback],\n","                precision=\"bf16-mixed\",\n","                gradient_clip_val=1.0\n","            )\n","\n","            print(f\"ðŸš€ Starting model training for {model_name.upper()}...\")\n","            trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","            print(f\"âœ… Training complete for {model_name.upper()}!\")\n","            print(f\"Best model for this run saved at: {checkpoint_callback.best_model_path}\")\n","            print(f\"{'='*20} FINISHED EXPERIMENT FOR MODEL: {model_name.upper()} {'='*20}\")\n","\n","    else:\n","        print(\"Dataset is too small to split. Aborting experiment.\")\n","\n","print(\"\\n--- All Advanced Model Comparison Experiments Complete ---\")"],"outputs":[],"execution_count":null,"metadata":{"id":"2RzhQadksJYT"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}