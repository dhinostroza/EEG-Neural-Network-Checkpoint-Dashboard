{"cells":[{"cell_type":"code","source":["# ==============================================================================\n","# SCRIPT TO TRAIN THE MODEL (DEFINITIVE, WITH WEIGHTED SAMPLER)\n","# This script uses an isolated environment, a single cached worker, and a\n","# WeightedRandomSampler to provide the most stable and balanced training.\n","# ==============================================================================\n","\n","import os\n","import sys\n","from google.colab import auth, drive\n","\n","# --- 1. SETUP THE COLAB ENVIRONMENT ---\n","print(\"--- Step 1: Preparing the main Colab environment ---\")\n","try:\n","    auth.authenticate_user()\n","    print(\"âœ… Authentication successful.\")\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"âœ… Google Drive mounted successfully.\")\n","except Exception as e:\n","    sys.exit(f\"âŒ FATAL ERROR: Could not set up environment. Details: {e}\")\n","\n","# --- 2. CREATE AND PROVISION THE ISOLATED TRAINING ENVIRONMENT ---\n","print(\"\\n--- Step 2: Creating and provisioning the isolated training environment ---\")\n","!pip install --upgrade -q virtualenv\n","!virtualenv train_env\n","!train_env/bin/pip install --upgrade -q pip \"pytorch-lightning\" \"timm\" \"pandas>=2.0\" \"pyarrow>=15.0\" \"fsspec>=2023.6.0\" gcsfs google-auth matplotlib seaborn scikit-learn\n","print(\"âœ… All dependencies installed successfully into 'train_env'.\")\n","\n","\n","# --- 3. CREATE AND RUN THE FULL TRAINING SCRIPT ---\n","print(\"\\n--- Step 3: Preparing and executing the training logic in the isolated environment ---\")\n","\n","python_script_logic = r'''\n","import matplotlib\n","matplotlib.use('Agg')\n","\n","import torch\n","import torch.nn as nn\n","import timm\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.data.sampler import WeightedRandomSampler\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import CSVLogger\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassConfusionMatrix\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from pathlib import Path\n","from collections import Counter\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report\n","\n","torch.set_float32_matmul_precision('medium')\n","print(\"âœ… Libraries imported inside virtual environment.\")\n","\n","# --- All class and function definitions are here ---\n","\n","def get_model(model_name='convnext_base', pretrained=True):\n","    model = timm.create_model('convnextv2_base.fcmae_ft_in22k_in1k', pretrained=pretrained)\n","    original_conv = model.stem[0]\n","    new_first_conv = nn.Conv2d(1, original_conv.out_channels, kernel_size=original_conv.kernel_size, stride=original_conv.stride, padding=original_conv.padding, bias=(original_conv.bias is not None))\n","    with torch.no_grad():\n","        if original_conv.weight.shape[1] == 3:\n","            new_first_conv.weight[:, :] = original_conv.weight.clone().mean(dim=1, keepdim=True)\n","    model.stem[0] = new_first_conv\n","    num_ftrs = model.head.fc.in_features\n","    model.head.fc = nn.Linear(num_ftrs, 5)\n","    return model\n","\n","class SleepStageClassifierLightning(pl.LightningModule):\n","    def __init__(self, model_name, learning_rate, class_weights, epochs):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = get_model(model_name=self.hparams.model_name)\n","        self.train_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.val_accuracy = MulticlassAccuracy(num_classes=5)\n","        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float) if class_weights else None)\n","    def forward(self, x): return self.model(x)\n","    def normalize_on_gpu(self, x):\n","        mean = torch.mean(x, dim=(1, 2, 3), keepdim=True)\n","        std = torch.std(x, dim=(1, 2, 3), keepdim=True)\n","        return (x - mean) / (std + 1e-6)\n","    def spec_augment(self, x, time_mask_param=10, freq_mask_param=10):\n","        _, _, num_freq_bins, num_time_steps = x.shape\n","        f_mask_width = int(np.random.uniform(0.0, freq_mask_param))\n","        f_mask_start = int(np.random.uniform(0.0, num_freq_bins - f_mask_width))\n","        x[:, :, f_mask_start:f_mask_start + f_mask_width, :] = 0\n","        t_mask_width = int(np.random.uniform(0.0, time_mask_param))\n","        t_mask_start = int(np.random.uniform(0.0, num_time_steps - t_mask_width))\n","        x[:, :, :, t_mask_start:t_mask_start + t_mask_width] = 0\n","        return x\n","    def training_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        x_normalized = self.normalize_on_gpu(x)\n","        x_augmented = self.spec_augment(x_normalized)\n","        y_pred_logits = self(x_augmented)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n","        self.log('train_acc', self.train_accuracy(y_pred_logits, y_true), on_epoch=True, prog_bar=True)\n","        return loss\n","    def validation_step(self, batch, batch_idx):\n","        x, y_true = batch\n","        x_normalized = self.normalize_on_gpu(x)\n","        y_pred_logits = self(x_normalized)\n","        loss = self.loss_fn(y_pred_logits, y_true)\n","        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n","        self.log('val_acc', self.val_accuracy(y_pred_logits, y_true), on_epoch=True, prog_bar=True)\n","        return loss\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n","        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.epochs, eta_min=1e-7)\n","        return [optimizer], [scheduler]\n","\n","class CombinedDataset(Dataset):\n","    def __init__(self, manifest_path, num_files=None):\n","        manifest_df = pd.read_csv(manifest_path)\n","        if num_files:\n","            manifest_df = manifest_df.head(num_files)\n","        self.file_paths = manifest_df['file_path'].tolist()\n","        self.cumulative_epochs = np.cumsum(manifest_df['epoch_count'].values)\n","        self.total_epochs = self.cumulative_epochs[-1]\n","        self._cache = {}\n","        print(f\"âœ… Dataset initialized from manifest. Found {self.total_epochs} epochs across {len(self.file_paths)} files.\")\n","    def __len__(self): return self.total_epochs\n","\n","    def get_labels_for_sampler(self):\n","        \"\"\"A new, truly efficient method to get all labels for the sampler.\"\"\"\n","        all_labels = []\n","        print(\"  -> Efficiently gathering all labels for sampler weighting...\")\n","        for i, file_path in enumerate(self.file_paths):\n","            if (i + 1) % 200 == 0 or i == len(self.file_paths) - 1:\n","                 print(f\"\\r     ...processing file {i+1}/{len(self.file_paths)}\", end=\"\")\n","            # --- MODIFICATION: Only read the 'label' column ---\n","            df_labels = pd.read_parquet(file_path, columns=['label'])\n","            labels = df_labels['label'][df_labels['label'].isin([0, 1, 2, 3, 4])].tolist()\n","            all_labels.extend(labels)\n","        print(\"\\n     ...done.\")\n","        return all_labels\n","\n","    def __getitem__(self, idx):\n","        file_idx = np.searchsorted(self.cumulative_epochs, idx, side='right')\n","        local_idx = idx - (self.cumulative_epochs[file_idx - 1] if file_idx > 0 else 0)\n","        file_path = self.file_paths[file_idx]\n","        if file_path not in self._cache:\n","            self._cache[file_path] = pd.read_parquet(file_path)[lambda df: df['label'].isin([0, 1, 2, 3, 4])].reset_index(drop=True)\n","        row = self._cache[file_path].iloc[local_idx]\n","        label = np.int64(row['label'])\n","        spectrogram_flat = row.drop('label').values.astype(np.float32)\n","        spectrogram_2d = spectrogram_flat.reshape(1, 76, 60)\n","        return torch.from_numpy(spectrogram_2d), torch.tensor(label)\n","\n","def generate_performance_report(ckpt_path, dataloader, device, save_dir, exp_name):\n","    pass # Placeholder\n","\n","# --- MAIN EXECUTION BLOCK ---\n","if __name__ == \"__main__\":\n","    GCS_MANIFEST_PATH = \"gs://shhs-sleepedfx-data-bucket/metadata/shhs_dataset_manifest.csv\"\n","    NUM_FILES_TO_USE = 2000\n","    CLASS_WEIGHTS = [0.7, 6.5, 0.5, 1.5, 1.2]\n","    EPOCHS = 40\n","    BATCH_SIZE = 256\n","    LEARNING_RATE = 2e-5\n","    NUM_WORKERS = 0\n","    DRIVE_CHECKPOINT_DIR = \"/content/drive/MyDrive/final_model_checkpoint/\"\n","\n","    full_dataset = CombinedDataset(GCS_MANIFEST_PATH, num_files=NUM_FILES_TO_USE)\n","    torch.manual_seed(42)\n","    train_size = int(0.8 * len(full_dataset))\n","    val_size = len(full_dataset) - train_size\n","    train_dataset_subset, val_dataset = random_split(full_dataset, [train_size, val_size])\n","\n","    print(\"\\n--- Creating a WeightedRandomSampler to address class imbalance ---\")\n","\n","    # Efficiently get all labels from the full dataset, then filter for the training subset\n","    all_labels_in_dataset = full_dataset.get_labels_for_sampler()\n","    train_subset_labels = [all_labels_in_dataset[i] for i in train_dataset_subset.indices]\n","\n","    class_counts = Counter(train_subset_labels)\n","    print(f\"  -> Training class distribution: {class_counts}\")\n","    class_weights_for_sampler = {i: 1.0 / count for i, count in class_counts.items()}\n","    sample_weights = [class_weights_for_sampler[label] for label in train_subset_labels]\n","\n","    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n","    print(\"âœ… Sampler created successfully.\")\n","\n","    # Use the sampler in the training DataLoader. shuffle MUST be False when using a sampler.\n","    train_loader = DataLoader(train_dataset_subset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    experiment_name = f\"{timestamp}_convnext_base_{NUM_FILES_TO_USE}files_Sampler_cwN1-{CLASS_WEIGHTS[1]}\"\n","\n","    model = SleepStageClassifierLightning('convnext_base', LEARNING_RATE, CLASS_WEIGHTS, EPOCHS)\n","\n","    drive_log_dir = \"/content/drive/MyDrive/sleep_logs/\"\n","    os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n","\n","    csv_logger = CSVLogger(drive_log_dir, name=experiment_name)\n","    checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=DRIVE_CHECKPOINT_DIR, filename=f\"best-model-{experiment_name}\", save_top_k=1, mode='min')\n","    early_stop_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=True, mode='min')\n","\n","    checkpoint_files = glob.glob(os.path.join(DRIVE_CHECKPOINT_DIR, \"*.ckpt\"))\n","    latest_checkpoint = None\n","    if checkpoint_files:\n","        latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n","        print(f\"âœ… Found checkpoint. Resuming from: {os.path.basename(latest_checkpoint)}\")\n","    else:\n","        print(\"  -> No checkpoint found. Starting a new training run.\")\n","\n","    trainer = pl.Trainer(\n","        max_epochs=EPOCHS, accelerator=\"gpu\", devices=1, logger=csv_logger,\n","        callbacks=[checkpoint_callback, early_stop_callback],\n","        precision=\"bf16-mixed\", gradient_clip_val=1.0\n","    )\n","\n","    print(f\"\\nðŸš€ðŸš€ðŸš€ Starting/Resuming training with sampler for experiment: {experiment_name} ðŸš€ðŸš€ðŸš€\")\n","    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=latest_checkpoint)\n","    print(f\"\\nâœ… Training complete!\")\n","\n","    if checkpoint_callback.best_model_path and os.path.exists(checkpoint_callback.best_model_path):\n","        print(\"Performance report generation placeholder\")\n","    else:\n","        print(\"  -> No checkpoint was saved.\")\n","'''\n","\n","# Write the script to a file\n","with open(\"run_training.py\", \"w\") as f:\n","    f.write(python_script_logic)\n","\n","# Execute the script\n","!MPLBACKEND=Agg train_env/bin/python run_training.py\n","\n","print(\"\\n--- Script execution finished. ---\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["--- Step 1: Preparing the main Colab environment ---\n","âœ… Authentication successful.\n","Mounted at /content/drive\n","âœ… Google Drive mounted successfully.\n","\n","--- Step 2: Creating and provisioning the isolated training environment ---\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hcreated virtual environment CPython3.12.11.final.0-64 in 243ms\n","  creator CPython3Posix(dest=/content/train_env, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==25.2\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n","âœ… All dependencies installed successfully into 'train_env'.\n","\n","--- Step 3: Preparing and executing the training logic in the isolated environment ---\n","âœ… Libraries imported inside virtual environment.\n","âœ… Dataset initialized from manifest. Found 2003013 epochs across 2000 files.\n","\n","--- Creating a WeightedRandomSampler to address class imbalance ---\n","  -> Efficiently gathering all labels for sampler weighting...\n"]}],"execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwQlLjgQLgNQ","outputId":"bfe388d4-170e-4b08-f64f-4d4691b54726"}}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}